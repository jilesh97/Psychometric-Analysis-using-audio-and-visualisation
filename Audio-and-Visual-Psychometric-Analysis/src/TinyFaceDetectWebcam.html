<!DOCTYPE html>
<html>

<head>
    <script src="../src/FileSaver.js"></script>
    <script src="../dist/face-api.js"></script>
    <script src="public/commons.js"></script>
    <link rel="stylesheet" href="public/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css">
    <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.14.1/dist/tf.min.js"></script>

<!--    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.3/dist/tf.min.js"> </script>-->
</head>

<body>
    <div id="navbar"></div>
    <div class="center-content page-container">
        <div class="progress" id="loader">
            <div class="indeterminate"></div>
        </div>
        <button id="save-btn">Save Button</button>
        <h5 >
            <span id="status">Model Loading ...</span>s
        </h5>
        <div style="position: relative" class="margin">
            <video onplay="onPlay(this)" id="inputVideo" autoplay muted></video>
            <canvas id="overlay" />
            <canvas id='canvas' />
        </div>
        <h5>Increase Input Size to get higher accuracy</h5>
        <center style="padding:10px">
            <div class="row input-field" style="margin-right: 20px;">

                <select id="sizeType">
                    <option value="" disabled selected>Input Size:</option>
                    <option value="160">160 x 160</option>
                    <option value="224">224 x 224</option>
                    <option value="320">320 x 320</option>
                    <option value="416">416 x 416</option>
                    <option value="608">608 x 608</option>
                </select>
                <label>Input Size </label>
            </div>

        </center>

        <!--
            <div class="row">
                <label for="scoreThreshold">Score Threshold:</label>
                <input disabled value="0.5" id="scoreThreshold" type="text" class="bold">
            </div>
-->
        <!--
            <button class="waves-effect waves-light btn" onclick="onDecreaseThreshold()">
                <i class="material-icons left">-</i>
            </button>
            <button class="waves-effect waves-light btn" onclick="onIncreaseThreshold()">
                <i class="material-icons left">+</i>
            </button>
-->
        <div class="row side-by-side">
            <div class="row">
                <label for="time">Time:</label>
                <input disabled value="-" id="time" type="text" class="bold">
            </div>
            <div class="row">
                <label for="fps">Estimated Fps:</label>
                <input disabled value="-" id="fps" type="text" class="bold">
            </div>
        </div>

    </div>
    <style>
        #status {
            font-family: Geneva, Tahoma, Verdana, sans-serif;
        }

        .inline {
            display: inline-block;
        }
    </style>
    <script>

        let scoreThreshold = 0.5
        let sizeType = '160'
        let modelLoaded = false
        var cImg;
        var constraints = {
            audio: false,
            video: {
                width: 640,
                height: 480
            }
        };
        var EmotionModel;
        var emotions = [];
        var oldLabel;
        var newline = '\n';
        var offset_x = 34;
        var offset_y = 20;
        var emotion_labels = ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"];
        var emotion_colors = ["#ff0000", "#00a800", "#ff4fc1", "#ffe100", "#306eff", "#ff9d00", "#7c7c7c"];
        debugger;
        emotions.push(getUrlParam('var1','Empty'));
        emotions.push("\n");
        console.log(emotions);
        emotions.push(getUrlParam('var2', 'Empty'));
        console.log(emotions);
        emotions.push(getUrlParam('var3', 'Empty'));
        console.log(emotions);
        emotions.push(getUrlParam('var4', 'Empty'));
        console.log(emotions);
        emotions.push(getUrlParam('var5', 'Empty'));

        emotions.push('\n');



        let forwardTimes = []

        function updateTimeStats(timeInMs) {
            forwardTimes = [timeInMs].concat(forwardTimes).slice(0, 30)
            const avgTimeInMs = forwardTimes.reduce((total, t) => total + t) / forwardTimes.length
            $('#time').val(`${Math.round(avgTimeInMs)} ms`)
            $('#fps').val(`${faceapi.round(1000 / avgTimeInMs)}`)
        }

        function onIncreaseThreshold() {
            scoreThreshold = Math.min(faceapi.round(scoreThreshold + 0.1), 1.0)
            $('#scoreThreshold').val(scoreThreshold)
        }

        function onDecreaseThreshold() {
            scoreThreshold = Math.max(faceapi.round(scoreThreshold - 0.1), 0.1)
            $('#scoreThreshold').val(scoreThreshold)
        }

        function onSizeTypeChanged(e, c) {
            sizeType = e.target.value
            $('#sizeType').val(sizeType)
        }

        async function onPlay(videoEl) {
            if (videoEl.paused || videoEl.ended || !modelLoaded)
                return false

            const {
                width,
                height
            } = faceapi.getMediaDimensions(videoEl)
            const canvas = $('#overlay').get(0)
            canvas.width = width
            canvas.height = height

            const forwardParams = {
                inputSize: parseInt(sizeType),
                scoreThreshold
            }

            const ts = Date.now()
            const result = await faceapi.detectAllFaces(videoEl, new faceapi.TinyFaceDetectorOptions(forwardParams))
            console.result
//            const result = await faceapi.tinyYolov2(videoEl, forwardParams)
            if (result.length != 0) {


                const context = canvas.getContext('2d')
                context.drawImage(videoEl, 0, 0, width, height)

                let ctx = context;
                ctx.lineWidth = 4;
                ctx.font = "25px Arial"
                ctx.fillText('Result', 0, 0);

                for (var i = 0; i < result.length; i++) {
                    ctx.beginPath();
                    var item = result[i].box;
                    let s_x = Math.floor(item._x+offset_x);
                    if (item.y<offset_y){
                        var s_y = Math.floor(item._y);
                    }
                    else{
                        var s_y = Math.floor(item._y-offset_y);
                    }
                    let s_w = Math.floor(item._width-offset_x);
                    let s_h = Math.floor(item._height);
                    let cT = ctx.getImageData(s_x, s_y, s_w, s_h);
                    cT = preprocess(cT);

                    z = EmotionModel.predict(cT)
                    let index = z.argMax(1).dataSync()[0]
                    let label = emotion_labels[index];
                    ///////
                    //var emotions = [] ;


                    if(oldLabel != label){
                        emotions.push(label + newline);
                    }
                    else{



                    }
                    oldLabel = label;
                    console.log(emotions);


                    ///////
                    ctx.strokeStyle = emotion_colors[index];
                    ctx.rect(s_x, s_y, s_w, s_h);
                    ctx.stroke();
                    ctx.fillStyle = emotion_colors[index];
                    ctx.fillText(label, s_x, s_y);
                    ctx.closePath();
                }

            }

            updateTimeStats(Date.now() - ts)

                       faceapi.drawDetection('overlay', result.map(det => det.forSize(width, height)), {
                           withScore: false
                       })
            setTimeout(() => onPlay(videoEl))
            var status = document.getElementById('status');
            status.innerHTML = "Running the model ... ";
        }

        async function loadNetWeights(uri) {
            return new Float32Array(await (await fetch(uri)).arrayBuffer())
        }
        // create model
        async function createModel(path) {
            let model = await tf.loadModel(path)
            return model
        }
        // load emotion model
        async function loadModel(path) {
            //            var lbl = document.getElementById("status");
            //            lbl.innerText = "Model Loading ..."
            //            let canvas = document.getElementById("combined");
            //            let cT = preprocess(cImg)
            EmotionModel = await createModel(path)
            //            z = model.predict(cT)
            //            toPixels(deprocess(z), canvas)
            //            lbl.innerText = "Model Loaded !"
        }

        function preprocess(imgData) {
            return tf.tidy(() => {
                let tensor = tf.fromPixels(imgData).toFloat();

                tensor = tensor.resizeBilinear([100, 100])

                tensor = tf.cast(tensor, 'float32')
                const offset = tf.scalar(255.0);
                // Normalize the image 
                const normalized = tensor.div(offset);
                //We add a dimension to get a batch shape 
                const batched = normalized.expandDims(0)
                return batched
            })
        }

        function successCallback(stream) {
            var videoEl = $('#inputVideo').get(0)
            videoEl.srcObject = stream;
        }

        function errorCallback(error) {
            alert(error)
            console.log("navigator.getUserMedia error: ", error);
            //            alert("navigator.getUserMedia error: ", error)
        }

        async function run() {
            const Model_url = '../models/tiny_face_detector/tiny_face_detector_model-weights_manifest.json'
            await faceapi.loadTinyFaceDetectorModel(Model_url)
            modelLoaded = true

            var status = document.getElementById('status');
            status.innerHTML = "Initializing the camera ... ";

            navigator.mediaDevices.getUserMedia(constraints)
                .then(successCallback)
                .catch(errorCallback);

            onPlay($('#inputVideo').get(0))
            $('#loader').hide()
        }

        function getUrlParam(parameter, defaultvalue){
            var urlparameter = defaultvalue;
            if(window.location.href.indexOf(parameter) > -1){
                urlparameter = getUrlVars()[parameter];
            }
            return urlparameter;
        }

        function getUrlVars() {
            var vars = {};
            var parts = window.location.href.replace(/[?&]+([^=&]+)=([^&]*)/gi, function(m,key,value) {
                vars[key] = value;
            });
            return vars;
        }

        $('#save-btn').click(function() {
            console.log(getUrlParam('var1','Empty'));
            var blob = new Blob(emotions, {type: "text/plain;charset=utf-8"})
            saveAs(blob,'newfile.txt')
            //download('D:/FrontEnd-EmotionDetection-master/', 'newfile.txt')
            //url, name, opts
        })

        $(document).ready(function() {
            loadModel('../models/mobilenetv1_models/model.json')
            const sizeTypeSelect = $('#sizeType')
            sizeTypeSelect.val(sizeType)
            sizeTypeSelect.on('change', onSizeTypeChanged)
            sizeTypeSelect.material_select()
            run()
        })

    </script>
</body>

</html>